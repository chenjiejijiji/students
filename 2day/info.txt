
程序会先构建/加载知识库，然后进入提问循环。

尝试提问基于你 knowledge.txt 内容的问题：

“RAG 是什么？”

“LangChain 的作用是什么？”

“向量数据库在 RAG 中有什么用？”

“Sentence Transformers 是做什么的？”

观察：

答案是否准确？

重点看 来源 部分！ 检索器找到的文本块是否真正与问题相关？这直接决定了生成答案的质量。

尝试问一些知识库之外的问题，看看系统如何反应（可能依赖LLM的通用知识，也可能胡说八道）。

尝试调整 k 值 (第6步的 search_kwargs={"k": 2})，比如改成1或3，观察答案和来源的变化。

第五步：实验与思考 (持续进行)

修改知识库： 编辑 data/knowledge.txt，添加、删除或修改内容。重新运行脚本（它会重新加载并处理这个文件）。问关于新内容的问题。

调整参数：

chunk_size (第3步): 尝试 100, 500, 1000。哪个大小对检索效果最好？

k (第6步): 返回更多或更少的上下文块。k=1 够吗？k=5 会引入不相关信息吗？

temperature (第7步): 调高 (如 0.7) 让答案更有创造性（可能更不准确），调低 (如 0) 让答案更确定。

尝试不同的 LLM：

注释掉 OpenAI 部分，取消注释 HuggingFaceHub 部分，试试 google/flan-t5-base 或其他模型 (需要 HF Token)。比较效果。

如果使用 OpenAI，试试 gpt-4 (如果可用)。

更换知识源：

尝试加载一个小的 PDF (PyPDFLoader) 或网页 (WebBaseLoader)。LangChain 支持多种 Document Loaders。(需要额外安装库如 pypdf)

理解流程： 打开 verbose=True (第8步)，观察 LangChain 执行每一步的详细日志。